#!/usr/bin/env python3
"""
Prepare Recipes for Vector DB Ingestion

Transforms processed recipe markdown files into a format optimized for
Pinecone vector database ingestion.

Usage:
    python scripts/prepare_for_vector_db.py              # Process all recipes
    python scripts/prepare_for_vector_db.py --limit 5    # Process first 5 recipes
    python scripts/prepare_for_vector_db.py --yes        # Skip confirmation
    python scripts/prepare_for_vector_db.py --no-cache   # Ignore LLM cache
    python scripts/prepare_for_vector_db.py --dry-run    # Parse only, no LLM calls
"""

import argparse
import json
import re
import sys
from pathlib import Path
from typing import TypedDict

# Add scripts directory to path for local imports
sys.path.insert(0, str(Path(__file__).parent))

from llm_classifier import (
    ClassificationResult,
    classify_recipe,
    get_discovered_enums,
)

# Paths
PROJECT_ROOT = Path(__file__).parent.parent
PROCESSED_RECIPES_DIR = PROJECT_ROOT / "data" / "processed-recipes"
OUTPUT_FILE = PROJECT_ROOT / "data" / "recipes_for_vector_db.py"
RATINGS_FILE = PROCESSED_RECIPES_DIR / "_ratings.json"


class RecipeRecord(TypedDict):
    _id: str
    content: str
    metadata: dict


def load_ratings() -> dict[str, float | str | None]:
    """Load ratings from the ratings JSON file."""
    if RATINGS_FILE.exists():
        return json.loads(RATINGS_FILE.read_text(encoding="utf-8"))
    return {}


def get_processed_recipes() -> list[Path]:
    """Get all processed recipe markdown files."""
    recipes = []
    for file in sorted(PROCESSED_RECIPES_DIR.glob("*.md")):
        # Skip special files
        if file.name.startswith("_"):
            continue
        recipes.append(file)
    return recipes


def parse_recipe_markdown(file_path: Path) -> dict:
    """
    Parse a processed recipe markdown file.

    Returns:
        Dict with: title, ingredients, instructions, rating, content
    """
    content = file_path.read_text(encoding="utf-8")
    lines = content.splitlines()

    result = {
        "title": "",
        "ingredients": "",
        "instructions": "",
        "rating": None,
        "content": content,
    }

    # Extract title from H1
    for line in lines:
        if line.startswith("# "):
            result["title"] = line[2:].strip()
            break

    # Extract rating
    for line in lines:
        if line.startswith("Rating:"):
            rating_text = line.replace("Rating:", "").strip()
            # Handle "N" (not tried), "[MISSING]", or "X/10" format
            if rating_text == "N" or rating_text == "[MISSING]":
                result["rating"] = None
            else:
                # Extract number from "X/10" format
                match = re.search(r"(\d+(?:\.\d+)?)", rating_text)
                if match:
                    result["rating"] = float(match.group(1))
            break

    # Extract ingredients section
    ingredients_lines = []
    in_ingredients = False
    for line in lines:
        if line.startswith("## Ingredients"):
            in_ingredients = True
            continue
        elif line.startswith("## ") and in_ingredients:
            break
        elif in_ingredients:
            ingredients_lines.append(line)

    result["ingredients"] = "\n".join(ingredients_lines).strip()

    # Extract instructions section
    instructions_lines = []
    in_instructions = False
    for line in lines:
        if line.startswith("## Instructions"):
            in_instructions = True
            continue
        elif line.startswith("## ") and in_instructions:
            break
        elif in_instructions:
            instructions_lines.append(line)

    result["instructions"] = "\n".join(instructions_lines).strip()

    return result


def create_record(
    recipe_id: str,
    parsed: dict,
    classification: ClassificationResult,
) -> RecipeRecord:
    """Create a vector DB record from parsed recipe and classification."""
    return {
        "_id": recipe_id,
        "content": parsed["content"],
        "metadata": {
            "title": parsed["title"],
            "ingredients": parsed["ingredients"],
            "instructions": parsed["instructions"],
            "rating": parsed["rating"],
            "diet": classification["diet"],
            "protein": classification["protein"],
            "cuisine": classification["cuisine"],
            "meal_type": classification["meal_type"],
            "difficulty": classification["difficulty"],
            "prepTimeMinutes": classification["prepTimeMinutes"],
        },
    }


def write_output_file(records: list[RecipeRecord], discovered: dict) -> None:
    """Write the output Python file with recipe records."""
    output = '''"""
Recipe records prepared for Pinecone vector database ingestion.

Generated by: scripts/prepare_for_vector_db.py
Schema: data/schemas/basicSchema.json

Usage:
    from data.recipes_for_vector_db import RECIPE_RECORDS
"""

RECIPE_RECORDS = [
'''

    for record in records:
        output += "    {\n"
        output += f'        "_id": {json.dumps(record["_id"])},\n'
        output += f'        "content": {json.dumps(record["content"])},\n'
        output += '        "metadata": {\n'

        metadata = record["metadata"]
        output += f'            "title": {json.dumps(metadata["title"])},\n'
        output += f'            "ingredients": {json.dumps(metadata["ingredients"])},\n'
        output += f'            "instructions": {json.dumps(metadata["instructions"])},\n'
        output += f'            "rating": {json.dumps(metadata["rating"])},\n'
        output += f'            "diet": {json.dumps(metadata["diet"])},\n'
        output += f'            "protein": {json.dumps(metadata["protein"])},\n'
        output += f'            "cuisine": {json.dumps(metadata["cuisine"])},\n'
        output += f'            "meal_type": {json.dumps(metadata["meal_type"])},\n'
        output += f'            "difficulty": {json.dumps(metadata["difficulty"])},\n'
        output += f'            "prepTimeMinutes": {json.dumps(metadata["prepTimeMinutes"])},\n'

        output += "        },\n"
        output += "    },\n"

    output += "]\n"

    # Add discovered enums as a comment/reference
    if discovered:
        output += "\n# New enum values discovered during classification:\n"
        output += "# (Consider adding these to the base schema)\n"
        output += f"# DISCOVERED_ENUMS = {json.dumps(discovered, indent=4)}\n"

    OUTPUT_FILE.write_text(output, encoding="utf-8")


def main():
    parser = argparse.ArgumentParser(
        description="Prepare processed recipes for vector database ingestion."
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=None,
        help="Limit the number of recipes to process (default: all)",
    )
    parser.add_argument(
        "--yes",
        "-y",
        action="store_true",
        help="Skip confirmation prompt",
    )
    parser.add_argument(
        "--no-cache",
        action="store_true",
        help="Ignore LLM classification cache (re-classify all)",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Parse recipes only, do not call LLM for classification",
    )

    args = parser.parse_args()

    # Get all processed recipes
    recipes = get_processed_recipes()

    if not recipes:
        print("No processed recipes found in", PROCESSED_RECIPES_DIR)
        sys.exit(1)

    # Apply limit
    if args.limit:
        recipes = recipes[: args.limit]

    # Confirmation prompt
    if not args.yes and not args.dry_run:
        print(f"\nAbout to process {len(recipes)} recipes.")
        print("This will make API calls to OpenAI GPT-4o for classification.")
        if not args.no_cache:
            print("(Cached classifications will be reused where available)")
        print()
        confirm = input("Continue? [y/N]: ").strip().lower()
        if confirm != "y":
            print("Aborted.")
            sys.exit(0)

    # Process recipes
    records = []
    failed = []

    print(f"\nProcessing {len(recipes)} recipes...\n")

    for i, recipe_path in enumerate(recipes, 1):
        recipe_id = recipe_path.stem
        print(f"[{i}/{len(recipes)}] {recipe_id}...", end=" ")

        try:
            # Parse the markdown
            parsed = parse_recipe_markdown(recipe_path)

            if args.dry_run:
                # Use placeholder classification for dry run
                classification: ClassificationResult = {
                    "diet": [],
                    "protein": [],
                    "cuisine": [],
                    "meal_type": [],
                    "difficulty": "medium",
                    "prepTimeMinutes": None,
                }
                print(f"parsed (dry-run)")
            else:
                # Classify with LLM
                classification = classify_recipe(
                    recipe_id,
                    parsed["content"],
                    use_cache=not args.no_cache,
                )
                print(f"classified -> {classification['cuisine']}, {classification['protein']}")

            # Create record
            record = create_record(recipe_id, parsed, classification)
            records.append(record)

        except Exception as e:
            print(f"FAILED: {e}")
            failed.append((recipe_id, str(e)))

    # Write output file
    if records:
        discovered = get_discovered_enums()
        write_output_file(records, discovered)

    # Summary
    print(f"\n{'=' * 50}")
    print("Summary:")
    print(f"  Processed: {len(records)}")
    print(f"  Failed: {len(failed)}")

    if failed:
        print("\nFailed recipes:")
        for recipe_id, error in failed:
            print(f"  - {recipe_id}: {error}")

    # Report discovered enums
    discovered = get_discovered_enums()
    if discovered:
        print("\nNew enum values discovered:")
        for field, values in discovered.items():
            print(f"  {field}: {', '.join(values)}")

    if records:
        print(f"\nOutput written to: {OUTPUT_FILE}")

    if args.dry_run:
        print("\n(Dry run - no LLM calls were made)")


if __name__ == "__main__":
    main()
